{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a spam filter for SMS messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to build a spam filter that classifies SMS messages as spams or hams (i.e., non-spam messages).\n",
    "\n",
    "The data set we will use to train and test our classifier is available in the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection).\n",
    "\n",
    "The [dataset](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) contains 5572 messages labelled as spam or non-spam (ham). \n",
    "\n",
    "We'll build the spam filter using the multinomial Naive Bayes Algorithm, and our goal is to reach an accuracy > 95%.\n",
    "\n",
    "In this project we'll do the following:\n",
    "- Split the dataset into a training set and a test set\n",
    "- Create a vocabulary of the unique words of all the SMS messages in our dataset\n",
    "- Convert the SMS messages into a word frequency matrix\n",
    "- Calculate the constants and parameters of the Naive Bayes Algorithm\n",
    "- Build the spam filter\n",
    "- Measure the accuracy of the spam filter\n",
    "\n",
    "The multinomial Naive Bayes algorithm we'll implement is the following:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}\n",
    "where $P(Spam | w_1,w_2, ..., w_n)$ is the probability a message is a spam, given its words, and $P(w_i|Spam)$ is the probability a word occurs in the message, given it is a spam. \n",
    "\n",
    "The same reasoning is then applied to estimate the probability a message is not a spam.\n",
    "\n",
    "\\begin{equation}\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset\n",
    "df=pd.read_table(\"https://raw.githubusercontent.com/Artur-Wanderley/Portfolio-of-my-data-science-projects/master/Building%20a%20Spam%20Filter%20with%20Naive%20Bayes/SMSSpamCollection\",sep=\"\\t\", header=None,names=[\"label\",\"SMS\"])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many rows and columns the dataset have?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    0\n",
       "SMS      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking for NaNs\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of non-spam and spam messages:\n",
      " ham     86.593683\n",
      "spam    13.406317\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Proportion of hams vs spams\n",
    "print(\"Percentage of non-spam and spam messages:\\n\",df[\"label\"].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, about 87% of the messages are non-spam (hams), whereas about 13% of the messages are spams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and test set\n",
    "\n",
    "Before we start building the spam filter, let's split our dataset into two sets, a training set, and a test set. With the training set, we'll build the spam filter, and with the test set, we'll evaluate how good the spam filter is to classify SMS messages as spams.\n",
    "\n",
    "We'll use about 80% of the dataset as the training set (4458 messages) and about 20% of the dataset as the test set (1114 messages). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4458\n",
      "1114\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into a training and a test set\n",
    "\n",
    "# Randomizing the dataset before splitting it \n",
    "random_df=df.sample(frac=1,random_state=1) \n",
    "\n",
    "# Creating the index to split the dataset\n",
    "split_index=round(len(random_df)*0.8)\n",
    "\n",
    "# Splitting the randomized dataset into training and test set\n",
    "training_set=random_df[:split_index].reset_index(drop=True)\n",
    "test_set=random_df[split_index:].reset_index(drop=True)\n",
    "\n",
    "# Checking the length of training and test set\n",
    "print(len(training_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check whether the percentage of non-spam (87%) vs spam (13%) we observed in the original dataset remains in the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     0.86541\n",
      "spam    0.13459\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Percentages of non-spam vs spam in the training set\n",
    "print(training_set[\"label\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     0.868043\n",
      "spam    0.131957\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Percentages of non-spam vs spam in the test set\n",
    "print(test_set[\"label\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the percentages of non-spam and spam messages remained practically the same in both the training and test set.\n",
    "\n",
    "Now, let's move on and do some data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data cleaning\n",
    " ## Remove punctuation and letter case\n",
    " \n",
    "To calculate the probabilities required by the Naive Bayes Algorithm, we'll remove the punctuation and letter case of the messages and convert the messages into lists of strings. This will facilitate extract the data to make it available for the algorithm.\n",
    "\n",
    "Here is how we are going to transform the data:\n",
    "\n",
    "![img](https://dq-content.s3.amazonaws.com/433/cpgp_dataset_3.png)\n",
    "\n",
    "We are going to convert the training set into a matrix of word frequency where each line is a message and the columns are the frequency of each word present in the SMS messages.\n",
    "\n",
    "Let's start by removing punctuation and letter case from the SMS messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                SMS\n",
       "0   ham                       yep  by the pretty sculpture\n",
       "1   ham      yes  princess  are you going to make me moan \n",
       "2   ham                         welp apparently he retired\n",
       "3   ham                                            havent \n",
       "4   ham  i forgot 2 ask ü all smth   there s a card on ..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing punctuation and letter case from the training set\n",
    "training_set[\"SMS\"]=training_set[\"SMS\"].str.replace(\"\\W\",\" \")\n",
    "training_set[\"SMS\"]=training_set[\"SMS\"].str.lower()\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a vocabulary of the SMS messages\n",
    "\n",
    "Remember from the top that the multinomial Naive Bayes Algorithm we're implementing requires the probability each word occurs in a spam message ($P(w_i|Spam)$). To calculate this probability, we'll need to create a vocabulary of all unique  words of the SMS messages and count them because:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation},\n",
    "\n",
    "where $N_{Vocabulary}$ is the number of unique words in all messages (both spam and ham).\n",
    "\n",
    "$N_{Vocabulary}$ is also required to calculate $P(w_i|Ham)$ because:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, let's create the vocabulary of unique words found in the SMS messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the vocabulary:\n",
      " 7783\n"
     ]
    }
   ],
   "source": [
    "# Transforming each SMS  message into a list of strings\n",
    "training_set[\"SMS\"]=training_set[\"SMS\"].str.split()\n",
    "\n",
    "# Creating the vocabulary\n",
    "vocabulary=[]\n",
    "\n",
    "for message in training_set[\"SMS\"]:\n",
    "    for word in message:\n",
    "        vocabulary.append(word)\n",
    "    \n",
    "# Removing duplicates from the vocabulary\n",
    "vocabulary=set(vocabulary) # This code remove duplicated words\n",
    "vocabulary=list(vocabulary)\n",
    "\n",
    "#Let's check how many words the vocabulary have\n",
    "print(\"Number of unique words in the vocabulary:\\n\",len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the SMS messages into a word frequency matrix\n",
    "\n",
    "Now that we have our vocabulary of 7783 unique words, let's convert our training set into a word-frequency matrix, as shown in the scheme above.\n",
    "\n",
    "Remember that in our matrix of word frequency, each line is a message and each column is a unique word of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of word counts per SMS\n",
    "word_counts_per_sms={unique_word:[0]*len(training_set[\"SMS\"]) for unique_word in vocabulary}\n",
    "\n",
    "# Counting the frequency of each word per SMS\n",
    "for index,sms in enumerate(training_set[\"SMS\"]):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>girl</th>\n",
       "      <th>jerk</th>\n",
       "      <th>stress</th>\n",
       "      <th>gudnite</th>\n",
       "      <th>subscriber</th>\n",
       "      <th>apes</th>\n",
       "      <th>century</th>\n",
       "      <th>probpop</th>\n",
       "      <th>reached</th>\n",
       "      <th>ecstasy</th>\n",
       "      <th>...</th>\n",
       "      <th>carpark</th>\n",
       "      <th>dept</th>\n",
       "      <th>straight</th>\n",
       "      <th>cat</th>\n",
       "      <th>snowboarding</th>\n",
       "      <th>oclock</th>\n",
       "      <th>06</th>\n",
       "      <th>fiend</th>\n",
       "      <th>pansy</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   girl  jerk  stress  gudnite  subscriber  apes  century  probpop  reached  \\\n",
       "0     0     0       0        0           0     0        0        0        0   \n",
       "1     0     0       0        0           0     0        0        0        0   \n",
       "2     0     0       0        0           0     0        0        0        0   \n",
       "3     0     0       0        0           0     0        0        0        0   \n",
       "4     0     0       0        0           0     0        0        0        0   \n",
       "\n",
       "   ecstasy  ...  carpark  dept  straight  cat  snowboarding  oclock  06  \\\n",
       "0        0  ...        0     0         0    0             0       0   0   \n",
       "1        0  ...        0     0         0    0             0       0   0   \n",
       "2        0  ...        0     0         0    0             0       0   0   \n",
       "3        0  ...        0     0         0    0             0       0   0   \n",
       "4        0  ...        0     0         0    0             0       0   0   \n",
       "\n",
       "   fiend  pansy  msg  \n",
       "0      0      0    0  \n",
       "1      0      0    0  \n",
       "2      0      0    0  \n",
       "3      0      0    0  \n",
       "4      0      0    0  \n",
       "\n",
       "[5 rows x 7783 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming the word counts dictionary into a dataset\n",
    "word_counts_per_sms=pd.DataFrame(word_counts_per_sms)\n",
    "word_counts_per_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>girl</th>\n",
       "      <th>jerk</th>\n",
       "      <th>stress</th>\n",
       "      <th>gudnite</th>\n",
       "      <th>subscriber</th>\n",
       "      <th>apes</th>\n",
       "      <th>century</th>\n",
       "      <th>probpop</th>\n",
       "      <th>...</th>\n",
       "      <th>carpark</th>\n",
       "      <th>dept</th>\n",
       "      <th>straight</th>\n",
       "      <th>cat</th>\n",
       "      <th>snowboarding</th>\n",
       "      <th>oclock</th>\n",
       "      <th>06</th>\n",
       "      <th>fiend</th>\n",
       "      <th>pansy</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                SMS  girl  jerk  \\\n",
       "0   ham                  [yep, by, the, pretty, sculpture]     0     0   \n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...     0     0   \n",
       "2   ham                    [welp, apparently, he, retired]     0     0   \n",
       "3   ham                                           [havent]     0     0   \n",
       "4   ham  [i, forgot, 2, ask, ü, all, smth, there, s, a,...     0     0   \n",
       "\n",
       "   stress  gudnite  subscriber  apes  century  probpop  ...  carpark  dept  \\\n",
       "0       0        0           0     0        0        0  ...        0     0   \n",
       "1       0        0           0     0        0        0  ...        0     0   \n",
       "2       0        0           0     0        0        0  ...        0     0   \n",
       "3       0        0           0     0        0        0  ...        0     0   \n",
       "4       0        0           0     0        0        0  ...        0     0   \n",
       "\n",
       "   straight  cat  snowboarding  oclock  06  fiend  pansy  msg  \n",
       "0         0    0             0       0   0      0      0    0  \n",
       "1         0    0             0       0   0      0      0    0  \n",
       "2         0    0             0       0   0      0      0    0  \n",
       "3         0    0             0       0   0      0      0    0  \n",
       "4         0    0             0       0   0      0      0    0  \n",
       "\n",
       "[5 rows x 7785 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the original training set to the word frequency matrix\n",
    "clean_training_set=pd.concat([training_set,word_counts_per_sms],axis=1)\n",
    "clean_training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the constants of the Naive Bayes Algorithm\n",
    "\n",
    "Now that our training set is ready, let's start building the Multinomial Naive Bayes Classifier.\n",
    "\n",
    "We'll start by calculating the constants $P(Spam)$, $P(ham)$, $N_{spam}$,$N_{Ham}$, and $N_{Vocabulary}$.\n",
    "\n",
    "Remember that the multinomial Naive Bayes Algorithm we are implementing to build the spam filter is the following.\n",
    "\n",
    "Probability of spam:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "Probability of ham:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating P(Ham) and P(Spam)\n",
    "p_ham=len(clean_training_set[clean_training_set[\"label\"]==\"ham\"])/clean_training_set.shape[0]\n",
    "p_spam=len(clean_training_set[clean_training_set[\"label\"]==\"spam\"])/clean_training_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in ham messages: 57237\n",
      "Number of words in spam messages: 15190\n"
     ]
    }
   ],
   "source": [
    "# Calculating N_ham\n",
    "cols=clean_training_set.iloc[:,2:].columns.tolist()\n",
    "n_ham=clean_training_set[clean_training_set[\"label\"]==\"ham\"][cols].values.sum()\n",
    "print(\"Number of words in ham messages:\",n_ham)\n",
    "\n",
    "# Calculating N_spam\n",
    "cols=clean_training_set.iloc[:,2:].columns.tolist()\n",
    "n_spam=clean_training_set[clean_training_set[\"label\"]==\"spam\"][cols].values.sum()\n",
    "print(\"Number of words in spam messages:\",n_spam)\n",
    "\n",
    "# n_vocabulary\n",
    "\n",
    "n_vocabulary=len(vocabulary)\n",
    "\n",
    "# Creating the alpha variable for Laplace smoothing\n",
    "alpha=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the parameters\n",
    "\n",
    "After calculating the constants of the Naive Bayes Algorithm, let's calculate the parameters $P(w_{i}|Spam)$ and $P(w_{i}|Ham)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating p_word_given_ham and p_word_given_spam\n",
    "p_ham_dic={unique_word:0 for unique_word in vocabulary}\n",
    "p_spam_dic={unique_word:0 for unique_word in vocabulary}\n",
    "ham_sms=clean_training_set[clean_training_set[\"label\"]==\"ham\"]\n",
    "spam_sms=clean_training_set[clean_training_set[\"label\"]==\"spam\"]\n",
    "\n",
    "for word in vocabulary:\n",
    "    n_word_given_ham=ham_sms[word].sum()\n",
    "    p_word_given_ham=(n_word_given_ham+alpha)/(n_ham+alpha*n_vocabulary)\n",
    "    p_ham_dic[word]=p_word_given_ham\n",
    "    \n",
    "    n_word_given_spam=spam_sms[word].sum()\n",
    "    p_word_given_spam=(n_word_given_spam+alpha)/(n_spam+alpha*n_vocabulary)\n",
    "    p_spam_dic[word]=p_word_given_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the spam filter\n",
    "\n",
    "Now that we have calculated all the constants and parameters, it's time to create our spam filter.\n",
    "\n",
    "The spam filter we'll create does the following:\n",
    "- Takes in a new message\n",
    "- Calculates the probability this new message is a ham ($P(Ham | w_1,w_2, ..., w_n)$) and the probability it is a spam ($P(Spam | w_1,w_2, ..., w_n)$)\n",
    " - If $P(Ham | w_1,w_2, ..., w_n) > P(Spam | w_1,w_2, ..., w_n)$, the message is classified as ham\n",
    " - If $P(Ham | w_1,w_2, ..., w_n) < P(Spam | w_1,w_2, ..., w_n)$, the message is classified as spam\n",
    " - If $P(Ham | w_1,w_2, ..., w_n)$ $=$ $P(Ham | w_1,w_2, ..., w_n)$, the algorithm will request human help to classify the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the spam filter\n",
    "def classify(message):\n",
    "    message=re.sub(\"\\W\",\" \",message) # Removes message punctuation\n",
    "    message=message.lower().split()\n",
    "    \n",
    "    p_spam_given_message=p_spam\n",
    "    p_ham_given_message=p_ham\n",
    "    \n",
    "    for word in message:\n",
    "        if word in p_spam_dic:\n",
    "            p_spam_given_message*=p_spam_dic[word]\n",
    "                   \n",
    "        if word in p_ham_dic:\n",
    "            p_ham_given_message*=p_ham_dic[word]         \n",
    "    \n",
    "    print(\"P(spam|message):\",p_spam_given_message)\n",
    "    print(\"P(ham|message):\",p_ham_given_message)\n",
    "    \n",
    "  \n",
    "    if p_spam_given_message > p_ham_given_message:\n",
    "        print(\"label: spam\")\n",
    "    \n",
    "    elif p_spam_given_message < p_ham_given_message:\n",
    "        print(\"label: ham\")\n",
    "    \n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the spam filter\n",
    "\n",
    "Let's try our spam filter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam|message): 1.3481290211300841e-25\n",
      "P(ham|message): 1.9368049028589875e-27\n",
      "label: spam\n"
     ]
    }
   ],
   "source": [
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam|message): 2.4372375665888117e-25\n",
      "P(ham|message): 3.687530435009238e-21\n",
      "label: ham\n"
     ]
    }
   ],
   "source": [
    "classify(\"Sounds good, Tom, then see u there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the spam filter accuracy\n",
    "\n",
    "Let's use the test set to measure how accurate our spam filter is.\n",
    "\n",
    "However, to test our spam filter, let's do some editing in the `classify` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Editing the spam filter code\n",
    "def classify(message):\n",
    "    message=re.sub(\"\\W\",\" \",message)\n",
    "    message=message.lower().split()\n",
    "    \n",
    "    p_spam_given_message=p_spam\n",
    "    p_ham_given_message=p_ham\n",
    "    \n",
    "    for word in message:\n",
    "        if word in p_spam_dic:\n",
    "            p_spam_given_message*=p_spam_dic[word]\n",
    "                   \n",
    "        if word in p_ham_dic:\n",
    "            p_ham_given_message*=p_ham_dic[word]         \n",
    "    \n",
    "#     print(\"P(spam|message):\",p_spam_given_message)\n",
    "#     print(\"P(ham|message):\",p_ham_given_message)\n",
    "    \n",
    "  \n",
    "    if p_spam_given_message > p_ham_given_message:\n",
    "        return \"spam\"\n",
    "    \n",
    "    elif p_spam_given_message < p_ham_given_message:\n",
    "        return \"ham\"\n",
    "    else:\n",
    "        return \"needs human classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1100\n",
      "Incorrect: 14\n",
      "Accuracy: 0.987\n"
     ]
    }
   ],
   "source": [
    "# Measuring the accuracy of our spam filter\n",
    "test_set[\"predicted\"]=test_set[\"SMS\"].apply(classify)\n",
    "\n",
    "correct_predictions=0\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    if test_set.loc[i,\"label\"]==test_set.loc[i,\"predicted\"]:\n",
    "        correct_predictions+=1\n",
    "        \n",
    "print(\"Correct:\",correct_predictions)\n",
    "print(\"Incorrect:\", len(test_set)-correct_predictions)\n",
    "print(\"Accuracy:\",round(correct_predictions/len(test_set),3))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The spam filter is nearly 99% accurate!\n",
    "\n",
    "As we can see above, of the 1114 messages in our test set, the spam correctly assigned 1000 SMS messages as spam ou ham.\n",
    "\n",
    "This is a pretty good performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
